{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 13 - Applying ESNs to the Lorenz System\n",
    "\n",
    "- In the lecture, we trained an ESN to reproduce the dynamics of the Lorenz system, correctly predicting the Lorenz trajectory up to ~10 Lyapunov times. After this time, the predicted trajectory deviates from the actual Lorenz trajectory, which is expected since small errors grow exponentially due to the chaotic dynamics of the Lorenz system. \n",
    "\n",
    "\n",
    "- Do the long-term dynamics of the ESN still constitute a valid Lorenz trajectory? Investigate this question by:\n",
    "    1. Plotting the attractor of the ESN for a long integration and comparing it qualitatively with the true Lorenz attractor.\n",
    "    2. Compute the maximum Lyapunov exponent of the ESN dynamics and compare it to the maximum Lyapunov exponent of the Lorenz system.\n",
    "    \n",
    "\n",
    "- Bonus: In the lecture, we performed a grid-search on the ESN hyperparameters and kept the model with the lowest error on the validation set. However, because the reservoir is generated randomly, there is some variance in the performance of an ESN for a given set of hyperparameters, meaning that the optimal hyperparameters chosen by cross-validation may not be detereministic. Extend the cross-validation procedure to generate and fit multiple ESNs for each combination of hyperparameters, keeping only the best one. Does this improve the prediction horizon of the ESN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DynamicalSystems, ReservoirComputing, Plots, Printf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESNHyperparams"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helper functions from the lecture\n",
    "\n",
    "\"\"\"\n",
    "    train_val_test_split(data; val_seconds, test_seconds, Δt = 0.1)\n",
    "\n",
    "Split the given data into training, validation, and test sets.\n",
    "\"\"\"\n",
    "function train_val_test_split(data; val_seconds, test_seconds, Δt = 0.1)\n",
    "    N = size(data, 2)\n",
    "    N_val = round(Int, val_seconds / Δt)\n",
    "    N_test = round(Int, test_seconds / Δt)\n",
    "    \n",
    "    ind1 = N - N_test - N_val\n",
    "    ind2 = N - N_test\n",
    "    \n",
    "    train_data = data[:, 1:ind1]\n",
    "    val_data = data[:, ind1+1:ind2]\n",
    "    test_data = data[:, ind2+1:end]\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    generate_esn(input_signal, reservoir_size = 1000, spectral_radius = 1.0, sparsity = 0.1, input_scale = 0.1)\n",
    "\n",
    "Generate an Echo State Network consisting of the reservoir weights W and the input weights Wᵢₙ.\n",
    "\"\"\"\n",
    "function generate_esn(input_signal, reservoir_size = 1000, spectral_radius = 1.0, sparsity = 0.1, input_scale = 0.1)\n",
    "    W = RandSparseReservoir(reservoir_size, radius = spectral_radius, sparsity = sparsity)\n",
    "    Wᵢₙ = WeightedLayer(scaling = input_scale)\n",
    "    return ESN(input_signal, reservoir = W, input_layer = Wᵢₙ)\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    train_esn!(esn, y, ridge_param)\n",
    "\n",
    "Given an Echo State Network, train it on the target sequence y_target and return the optimised output weights Wₒᵤₜ.\n",
    "\"\"\"\n",
    "function train_esn!(esn, y_target, ridge_param)\n",
    "    training_method = StandardRidge(ridge_param)\n",
    "    return train(esn, y_target, training_method)\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    cross_validate_esn(train_data, val_data, param_grid)\n",
    "\n",
    "Do a grid search on the given param_grid to find the optimal hyperparameters.\n",
    "\"\"\"\n",
    "function cross_validate_esn(train_data, val_data, param_grid; iters = 1)\n",
    "    best_loss = Inf\n",
    "    best_params = nothing\n",
    "    best_esn = nothing\n",
    "\n",
    "    # We want to predict one step ahead, so the input signal is equal to the target signal from the previous step\n",
    "    u_train = train_data[:, 1:end-1]\n",
    "    y_train = train_data[:, 2:end]\n",
    "        \n",
    "    for hyperparams in param_grid\n",
    "        # Unpack the hyperparams struct\n",
    "        (;reservoir_size, spectral_radius, sparsity, input_scale, ridge_param) = hyperparams\n",
    "\n",
    "        for _ in 1:iters\n",
    "            # Generate and train an ESN\n",
    "            esn = generate_esn(u_train, reservoir_size, spectral_radius, sparsity, input_scale)\n",
    "            Wₒᵤₜ = train_esn!(esn, y_train, ridge_param)\n",
    "\n",
    "            # Evaluate the loss on the validation set\n",
    "            steps_to_predict = size(val_data, 2)\n",
    "            prediction = esn(Generative(steps_to_predict), Wₒᵤₜ)\n",
    "            loss = sum(abs2, prediction - val_data)\n",
    "\n",
    "            # Keep track of the best hyperparameter values\n",
    "            if loss < best_loss\n",
    "                best_loss = loss\n",
    "                best_params = hyperparams\n",
    "                best_esn = esn\n",
    "                println(hyperparams)\n",
    "                @printf \"Validation loss = %.1e\\n\" best_loss\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Retrain the model using the optimal hyperparameters on both the training and validation data\n",
    "    # This is necessary because we don't want errors incurred during validation to affect the test error\n",
    "    (;reservoir_size, spectral_radius, sparsity, input_scale, ridge_param) = best_params\n",
    "    data = hcat(train_data, val_data)\n",
    "    u = data[:, 1:end-1]\n",
    "    y = data[:, 2:end]\n",
    "    esn = ESN(u, reservoir = best_esn.reservoir_matrix, input_layer = best_esn.input_matrix)\n",
    "    Wₒᵤₜ = train_esn!(esn, y, ridge_param)\n",
    "    \n",
    "    return esn, Wₒᵤₜ\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    plot_prediction(esn, Wₒᵤₜ, test_data, λ_max)\n",
    "\n",
    "Given an Echo State Network, plot its predictions versus the given test set.\n",
    "\"\"\"\n",
    "function plot_prediction(esn, Wₒᵤₜ, test_data, λ_max)\n",
    "    steps_to_predict = size(test_data, 2)\n",
    "    prediction = esn(Generative(steps_to_predict), Wₒᵤₜ)\n",
    "    \n",
    "    label = [\"actual\" \"predicted\"]\n",
    "    times = Δt * collect(0:steps_to_predict)[1:end-1] / λ_max\n",
    "\n",
    "    p1 = plot(times, [test_data[1, :], prediction[1, :]], label = label, ylabel = \"x(t)\")\n",
    "    p2 = plot(times, [test_data[2, :], prediction[2, :]], label = label, ylabel = \"y(t)\")\n",
    "    p3 = plot(times, [test_data[3, :], prediction[3, :]], label = label, ylabel = \"z(t)\", xlabel = \"t * λ_max\")\n",
    "    plot(p1, p2, p3, layout = (3, 1), size = (800, 600))\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Hyperparameters for an Echo State Network.\n",
    "\"\"\"\n",
    "struct ESNHyperparams\n",
    "    reservoir_size\n",
    "    spectral_radius\n",
    "    sparsity\n",
    "    input_scale\n",
    "    ridge_param\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×2001 Matrix{Float64}:\n",
       "  5.22753  10.7286  15.3066   9.28373   …  -6.19352  -1.21208   0.420035\n",
       "  9.14907  17.4786  14.9175   0.533337      1.28217   1.57029   1.34964\n",
       " 12.8547   19.4383  36.6727  36.3105       32.6602   24.5493   18.7748"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the Lorenz system\n",
    "ds = Systems.lorenz63()\n",
    "\n",
    "# Compute the maximum Lyapunov exponent of the system (we will use this to benchmark the model predictions later)\n",
    "λ_max = lyapunov(ds, 100, Ttr = 100)\n",
    "\n",
    "# Integrate the system\n",
    "T = 200.0\n",
    "Δt = 0.1\n",
    "Ttr = 100.0\n",
    "\n",
    "tr = trajectory(ds, T, Δt = Δt, Ttr = Ttr)\n",
    "x, y, z = columns(tr)\n",
    "\n",
    "# Put the data in the matrix format (n_features, n_samples) required by ReservoirComputing.jl \n",
    "data = reduce(vcat, [x, y, z]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "- We do the bonus part of the exercise first and experiment with alternative cross-validation techniques.\n",
    "\n",
    "\n",
    "- In particular, we should point out two important modifications to the method `cross_validate_esn` compared to the one we used in the lecture:\n",
    "    1. We now reuse the reservoir connections matrix $\\mathbf W$ and the input matrix $\\mathbf W^{\\mathrm{in}}$ of the best model. In the lecture, we simply stored the hyperparameters of the best model and used them to generate a new ESN, and in turn a new $\\mathbf W$ and the input matrix $\\mathbf W^{\\mathrm{in}}$. However, due to the randomness in the ESN generation, it's possible that this new reservoir would turn out to be suboptimal.\n",
    "    2. For each combination of hyperparameters, we now have the option of generating and evaluating multiple ESNs, to account for the possibility of individual ESNs which perform poorly. This is controlled by the `iters` keyword argument of `cross_validate_esn`.\n",
    "    \n",
    "   \n",
    "- To keep the runtime to a minimum, we will only optimise the spectral radius of the reservoir, but feel free to experiment with other hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 hyperparameter combinations.\n"
     ]
    }
   ],
   "source": [
    "# Set up the hyperparameter grid-search\n",
    "param_grid = []\n",
    "\n",
    "reservoir_sizes = [1024]\n",
    "spectral_radii = [0.6, 0.8, 1.0, 1.2]\n",
    "sparsities = [0.05]\n",
    "input_scales = [0.1]\n",
    "ridge_values = [0.0]\n",
    "\n",
    "# Take the Cartesian product of the possible values\n",
    "for params in Iterators.product(reservoir_sizes, spectral_radii, sparsities, input_scales, ridge_values)\n",
    "    push!(param_grid, ESNHyperparams(params...))\n",
    "end\n",
    "\n",
    "println(length(param_grid), \" hyperparameter combinations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = train_val_test_split(data, val_seconds = 15 / λ_max, test_seconds = 15 / λ_max);\n",
    "@time esn, Wₒᵤₜ = cross_validate_esn(train_data, val_data, param_grid, iters = 5)\n",
    "plot_prediction(esn, Wₒᵤₜ, test_data, λ_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Based on our experiments, it is in general difficult to obtain a meaningful prediction beyond ~10 Lyapunov times, although we did observe occasional models capable of predicting up to 13-14 Lyapunov times. Once again, feel free to experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Lorenz Attractor of the ESN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a trajectory from the trained ESN\n",
    "steps_to_predict = length(tr)\n",
    "predicted_trajectory = esn(Generative(steps_to_predict), Wₒᵤₜ)\n",
    "predicted_trajectory = Dataset(predicted_trajectory')  # Put the data in a Dataset object for DynamicalSystems.jl "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: It doesn't matter that this trajectory has different initial conditions from the ground truth trajectory we generated earlier, as we are only interested in the qualitative shape of the attractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the Lorenz attractor of the predicted trajectory to the actual trajectory we computed earlier\n",
    "p1 = plot(tr[:, 1], tr[:, 2], tr[:, 3], xlabel = \"x(t)\", ylabel = \"y(t)\", zlabel = \"z(t)\", label = \"Actual\")\n",
    "p2 = plot(predicted_trajectory[:, 1], predicted_trajectory[:, 2], predicted_trajectory[:, 3], xlabel = \"x(t)\", ylabel = \"y(t)\", zlabel = \"z(t)\", label = \"Predicted\", color = :orange)\n",
    "plot(p1, p2, layout = (1, 2), size = (900, 500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- While the timestep is relatively large, rendering the plot not very smooth, the predicted attractor looks qualitatively reasonable, albeit not perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the Maximum Lyapunov Exponent from the Trained ESN\n",
    "\n",
    "- For simplicity, we will reuse the method from exercise 6 for estimating Lyapunov exponents from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CairoMakie\n",
    "\n",
    "\"\"\"Helper function to compute the Lyapunov exponent and plot k x Δt versus E(k) - E(0).\"\"\"\n",
    "function estimate_lyapunov(data, ks, Δt)    \n",
    "    E = lyapunov_from_data(data, ks)   # Returns [E(k) for k ∈ ks]\n",
    "    t = ks .* Δt\n",
    "    (ind1, ind2), λ = linear_region(t, E)\n",
    "    \n",
    "    println(\"Identified linear region between t = \", t[ind1], \" and t = \", t[ind2])\n",
    "    println(\"Estimated Lyapunov exponent λ = \", round(λ, digits = 3))\n",
    "    \n",
    "    # Plotting\n",
    "    xlim = maximum(ks .* Δt)\n",
    "    ylim = maximum(E .- E[1])\n",
    "    xticks = 0:ks.step:xlim\n",
    "    yticks=0:1:ylim\n",
    "    fig = CairoMakie.Figure(figsize=(500,500))\n",
    "    ax = CairoMakie.Axis(fig[1, 1]; xlabel=\"k x Δt\", ylabel=\"E(k) - E(0)\", xticks, yticks)\n",
    "    CairoMakie.lines!(ax, ks .* Δt, E .- E[1], label = \"λ=$(round(λ, digits = 3))\")\n",
    "    CairoMakie.axislegend(ax, position = :rb)\n",
    "    fig\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a long trajectory from the trained ESN\n",
    "steps_to_predict = 10000\n",
    "predicted_trajectory = esn(Generative(steps_to_predict), Wₒᵤₜ)\n",
    "predicted_trajectory = Dataset(predicted_trajectory')  # Put the data in a Dataset object for DynamicalSystems.jl \n",
    "\n",
    "# Estimate the Lyapunov exponent\n",
    "ks = 0:1:100\n",
    "Δt = 0.1\n",
    "estimate_lyapunov(predicted_trajectory, ks, Δt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compared to the true Lyapunov exponent $\\lambda_{\\mathrm{max}} \\approx 0.92$, this isn't bad at all. With a more robust hyperparameter search perhaps we could do even better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
