{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0721ddf3",
   "metadata": {},
   "source": [
    "# Exercise 11 \n",
    "\n",
    "## Neural ODEs modelling the Lorenz System \n",
    "\n",
    "* Generate training data from a long trajectory of the Lorenz 63 system \n",
    "* Try fitting a Neural ODE to this training data\n",
    "    * \"Forget\" some of the terms of the $y$-coordinate of the Lorenz 63 and replace that with an ANN\n",
    "    * It is best to start the training with (very) short integration intervals (i.e. `tspans`) \n",
    "    * Either adjust some of the scripts given in the lecture, or look into the many example scripts of `DiffEqFlux.jl`s [documentation](https://diffeqflux.sciml.ai/dev/)\n",
    "* Make a long forecast with your Neural ODE and compare it to the true Lorenz 63 system\n",
    "    * For how many time steps is a meaningful forecast possible? \n",
    "    * Scale the forecast length with the maximum Lyapunov exponent and the step size `dt` to get a natural time scale of the system\n",
    "    * Try changing some of the parameters of your network and the length of the training set and see how it impacts the forecast length\n",
    "\n",
    "## Solution\n",
    "\n",
    "You can find a script implementing a hybrid Lorenz 63 setup [here](https://github.com/maximilian-gelbrecht/ChaoticNDETools.jl/tree/main/scripts/l63), you'll also find a hyperparameter optimization there. The hyperparameter is done to run on a high performance computer cluster, but you can easily modify to run it on a local computer. Obviously you can't run as many trials on your notebook though."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
