{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 12 - Applying ESNs to the Lorenz System\n",
    "\n",
    "- In the lecture, we trained an ESN to reproduce the dynamics of the Lorenz system, correctly predicting the Lorenz trajectory up to ~10 Lyapunov times. After this time, the predicted trajectory deviates from the actual Lorenz trajectory, which is expected since small errors grow exponentially due to the chaotic dynamics of the Lorenz system. \n",
    "<br>\n",
    "\n",
    "- Do the long-term dynamics of the ESN still constitute a valid Lorenz trajectory? Investigate this question by:\n",
    "    1. Plotting the attractor of the ESN for a long integration and comparing it qualitatively with the true Lorenz attractor.\n",
    "    2. Compute the maximum Lyapunov exponent of the ESN dynamics and compare it to the maximum Lyapunov exponent of the Lorenz system.<br><br>    \n",
    "\n",
    "- Bonus: In the lecture, we performed a grid-search on the ESN hyperparameters and kept the model with the lowest error on the validation set. However, because the reservoir is generated randomly, there is some variance in the performance of an ESN for a given set of hyperparameters, meaning that the optimal hyperparameters chosen by cross-validation may not be deterministic. Extend the cross-validation procedure to generate and fit multiple ESNs for each combination of hyperparameters, keeping only the best one. Does this improve the prediction horizon of the ESN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\")\n",
    "Pkg.add([\"DynamicalSystems\", \"ReservoirComputing\", \"Plots\", \"Printf\", \"MKL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DynamicalSystems, ReservoirComputing, Plots, Printf, MKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions from the lecture\n",
    "\n",
    "\"\"\"\n",
    "    train_val_test_split(data; val_seconds, test_seconds, Δt = 0.1)\n",
    "\n",
    "Split the given data into training, validation, and test sets.\n",
    "\"\"\"\n",
    "function train_val_test_split(data; val_seconds, test_seconds, Δt = 0.1)\n",
    "    N = size(data, 2)\n",
    "    N_val = round(Int, val_seconds / Δt)\n",
    "    N_test = round(Int, test_seconds / Δt)\n",
    "    \n",
    "    ind1 = N - N_test - N_val\n",
    "    ind2 = N - N_test\n",
    "    \n",
    "    train_data = data[:, 1:ind1]\n",
    "    val_data = data[:, ind1+1:ind2]\n",
    "    test_data = data[:, ind2+1:end]\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    train_esn!(esn, y, ridge_param)\n",
    "\n",
    "Given an Echo State Network, train it on the target sequence y_target and return the optimised output weights Wₒᵤₜ.\n",
    "\"\"\"\n",
    "function train_esn!(esn, y_target, ridge_param)\n",
    "    training_method = StandardRidge(ridge_param)\n",
    "    return train(esn, y_target, training_method)\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    cross_validate_esn(train_data, val_data, param_grid)\n",
    "\n",
    "Do a grid search on the given param_grid to find the optimal hyperparameters.\n",
    "\"\"\"\n",
    "function cross_validate_esn(train_data, val_data, param_grid)\n",
    "    best_loss = Inf\n",
    "    best_params = nothing\n",
    "\n",
    "    # We want to predict one step ahead, so the input signal is equal to the target signal from the previous step\n",
    "    # i.e. the sequence is shifted by one step\n",
    "    u_train = train_data[:, 1:end-1]\n",
    "    y_train = train_data[:, 2:end]\n",
    "        \n",
    "    for hyperparams in param_grid        \n",
    "        # Unpack the hyperparams struct\n",
    "        (; reservoir_size, spectral_radius, sparsity, input_scale, ridge_param) = hyperparams\n",
    "\n",
    "        # Generate and train an ESN\n",
    "        esn = ESN(\n",
    "            u_train,\n",
    "            3,\n",
    "            reservoir_size;\n",
    "            reservoir=rand_sparse(; radius=spectral_radius, sparsity=sparsity),\n",
    "            input_layer=scaled_rand(; scaling=input_scale),\n",
    "        )\n",
    "        Wₒᵤₜ = train_esn!(esn, y_train, ridge_param)\n",
    "\n",
    "        # Evaluate the loss on the validation set\n",
    "        steps_to_predict = size(val_data, 2)\n",
    "        prediction = esn(Generative(steps_to_predict), Wₒᵤₜ)\n",
    "        loss = sum(abs2, prediction - val_data)\n",
    "        \n",
    "        # Keep track of the best hyperparameter values\n",
    "        if loss < best_loss\n",
    "            best_loss = loss\n",
    "            best_params = hyperparams\n",
    "            println(best_params)\n",
    "            @printf \"Validation loss = %.1e\\n\" best_loss\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Retrain the model using the optimal hyperparameters on both the training and validation data\n",
    "    # This is necessary because we don't want errors incurred during validation to affect the test error\n",
    "    (;reservoir_size, spectral_radius, sparsity, input_scale, ridge_param) = best_params\n",
    "    data = hcat(train_data, val_data)\n",
    "    u = data[:, 1:end-1]\n",
    "    y = data[:, 2:end]\n",
    "    esn = ESN(\n",
    "        u,\n",
    "        3,\n",
    "        reservoir_size;\n",
    "        reservoir=rand_sparse(; radius=spectral_radius, sparsity=sparsity),\n",
    "        input_layer=scaled_rand(; scaling=input_scale),\n",
    "    )\n",
    "    Wₒᵤₜ = train_esn!(esn, y, ridge_param)\n",
    "    \n",
    "    return esn, Wₒᵤₜ\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    plot_prediction(esn, Wₒᵤₜ, test_data, λ_max)\n",
    "\n",
    "Given an Echo State Network, plot its predictions versus the given test set.\n",
    "\"\"\"\n",
    "function plot_prediction(esn, Wₒᵤₜ, test_data, λ_max)\n",
    "    steps_to_predict = size(test_data, 2)\n",
    "    prediction = esn(Generative(steps_to_predict), Wₒᵤₜ)\n",
    "    \n",
    "    label = [\"actual\" \"predicted\"]\n",
    "    times = Δt * collect(0:steps_to_predict)[1:end-1] / λ_max\n",
    "\n",
    "    p1 = plot(times, [test_data[1, :], prediction[1, :]], label = label, ylabel = \"x(t)\")\n",
    "    p2 = plot(times, [test_data[2, :], prediction[2, :]], label = label, ylabel = \"y(t)\")\n",
    "    p3 = plot(times, [test_data[3, :], prediction[3, :]], label = label, ylabel = \"z(t)\", xlabel = \"t * λ_max\")\n",
    "    plot(p1, p2, p3, layout = (3, 1), size = (800, 600))\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Hyperparameters for an Echo State Network.\n",
    "\"\"\"\n",
    "struct ESNHyperparams\n",
    "    reservoir_size\n",
    "    spectral_radius\n",
    "    sparsity\n",
    "    input_scale\n",
    "    ridge_param\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.6",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
